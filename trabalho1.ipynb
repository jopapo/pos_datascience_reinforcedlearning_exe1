{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Trabalho Prático I - Problema de Transporte de Objeto\r\n",
    "\r\n",
    "Descrição na [ementa](enunciado_trabalho_I_v2.pdf).\r\n",
    "\r\n",
    "Neste cenário, um agente deve percorrer a grade 7x6, encontrar o objeto e transportá-lo até na base. Essa tarefa deve ser executada na menor quantidade de passos detempo possível. O agente não possui nenhum conhecimento prévio sobre o ambiente, o qual possui paredes, as quais ele não pode transpor. O agente também não possui conhecimento prévio sobre a localização do objeto. A localização inicial do agente, disposição das paredes e objeto são sempre fixas, conforme indicado na ilustração. A cada passo de tempo, o agente pode executar os seguintes movimentos na grade:\r\n",
    "- mover para cima, baixo, esquerda ou direita;\r\n",
    "- permanecer na mesma célula;\r\n",
    "\r\n",
    "Este cenário apresenta algumas restrições de movimentação:\r\n",
    "- O agente pode realizar apenas uma movimentação por passo de tempo;\r\n",
    "- Se o agente escolher se mover para uma célula que não está vazia, seja por conta de uma parede ou objeto, ele não se move, i.e., permanece na mesma célula;\r\n",
    "- Qualquer tentativa de locomoção para além da grade, resultará na não movimentação do agente;\r\n",
    "- O objeto sé pode ser agarrado pela sua esquerda ou direita;\r\n",
    "- Quando o agente ́e posicionado à direita ou esquerda do objeto, o objeto ée agarrado automaticamente;\r\n",
    "- Uma vez agarrado o objeto, o agente não pode soltá-lo;\r\n",
    "- O agente, quando agarrado ao objeto, só consegue se mover para uma nova célula desde que não haja nenhuma restrição de movimentação para o agente e objeto;\r\n",
    "\r\n",
    "O episódio ée concluído automaticamente quando o objeto entra na base ou se atingir um número máximo de passos de tempo sem resolver a tarefa. Em ambos os casos, um novo episódio ́é iniciado, com o agente e objeto situados conforme abaixo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "source": [
    "# Libs necessária\r\n",
    "import gym\r\n",
    "from gym.envs.toy_text import discrete\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "#import random\r\n",
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "source": [
    "# Montando o cenário \r\n",
    "# Exemplo 1: https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e\r\n",
    "# Exemplo 2: https://github.com/caburu/gym-cliffwalking/blob/master/gym_cliffwalking/envs/cliffwalking_env.py\r\n",
    "# Exemplo 3: https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\r\n",
    "\r\n",
    "class ObjectTransportEnv(discrete.DiscreteEnv):\r\n",
    "    \"\"\"\r\n",
    "    O mapa é descrito com:\r\n",
    "    _ : caminho livre\r\n",
    "    o : posição inicial do agente\r\n",
    "    + : objeto a ser transportado\r\n",
    "    $ : objetivo\r\n",
    "    # : bloqueio\r\n",
    "    O episódio termina chegar no objetivo.\r\n",
    "    Sua recompensa é 1 se pegar o objeto e 1 se chegar ao objetivo com o objeto.\r\n",
    "    Fora do mapa não é acessível. Mais regras na ementa acima.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    metadata = {\"render.modes\": [\"human\", \"ansi\"]}\r\n",
    "    l_free_path = b'_'\r\n",
    "    l_agent = b'o'\r\n",
    "    l_object = b'+'\r\n",
    "    l_goal = b'$'\r\n",
    "    l_wall = b'#'\r\n",
    "    actions_movements = [\r\n",
    "        (0, -1) # left\r\n",
    "        ,(-1, 0) # up\r\n",
    "        ,(0, 1) # rigth\r\n",
    "        ,(1, 0) # down\r\n",
    "    ]\r\n",
    "    steps = []\r\n",
    "\r\n",
    "    def __init__(self, desc):\r\n",
    "        self.desc = np.asarray(desc, dtype=\"c\")\r\n",
    "        self.nrow, self.ncol = self.desc.shape\r\n",
    "        self.reward_range = (-100, 100)\r\n",
    "\r\n",
    "        # 4 ações (cima, baixo, esquerda, direita)\r\n",
    "        nA = len(self.actions_movements)\r\n",
    "        # Espaço é o tamanho do mapa\r\n",
    "        nS = self.nrow * self.ncol\r\n",
    "\r\n",
    "        # estado inicial\r\n",
    "        isd = np.array(self.desc == self.l_agent).astype(\"float64\").ravel()\r\n",
    "        isd /= isd.sum()\r\n",
    "\r\n",
    "        # lista com transições (probability, nextstate, reward, done)\r\n",
    "        P = {s: {} for s in range(nS)}\r\n",
    "        \r\n",
    "        for row in range(self.nrow):\r\n",
    "            for col in range(self.ncol):\r\n",
    "                s = self._to_s(row, col)\r\n",
    "                # movimento protegido para não ir pra fora da lista\r\n",
    "                #for action, predicted_pos in self._possible_actions(row, col):\r\n",
    "                for action, predicted_pos in self._possible_actions(row, col):\r\n",
    "                    P[s][action] = [(1.0, *self._update_probability_matrix(predicted_pos))]\r\n",
    "\r\n",
    "        super(ObjectTransportEnv, self).__init__(nS, nA, P, isd)\r\n",
    "\r\n",
    "    def _inc(self, row, col, action):\r\n",
    "        inc_row, inc_col = action\r\n",
    "        col = col + inc_col\r\n",
    "        row = row + inc_row\r\n",
    "        return (row, col)\r\n",
    "\r\n",
    "    def _to_s(self, row, col):\r\n",
    "        return row * self.ncol + col\r\n",
    "\r\n",
    "    def _s_to_row_col(self, s = None):\r\n",
    "        _s = s or self.s\r\n",
    "        return _s // self.ncol, _s % self.ncol\r\n",
    "\r\n",
    "    def _possible_actions(self, row, col):\r\n",
    "        for index in range(len(self.actions_movements)):\r\n",
    "            newpos = self._inc(row, col, self.actions_movements[index])\r\n",
    "            if self._is_valid_pos(newpos):\r\n",
    "                yield index, newpos\r\n",
    "    \r\n",
    "    def _is_valid_pos(self, pos):\r\n",
    "        row, col = pos\r\n",
    "        inside = row >= 0 and col >= 0 and row < self.nrow and col < self.ncol\r\n",
    "        return inside\r\n",
    "\r\n",
    "        # Teste pra ver se é melhor ignorar a parede ou dar uma pontuação negativa\r\n",
    "        #walls = False\r\n",
    "        #if inside:\r\n",
    "        #    walls = self.desc[row, col] == self.l_wall\r\n",
    "        #return inside and not walls\r\n",
    "\r\n",
    "    def is_valid_current_action(self, action):\r\n",
    "        return action in self.P[self.s]\r\n",
    "\r\n",
    "    def sample_from_available_current_actions(self):\r\n",
    "        keys = list(self.P[self.s].keys())\r\n",
    "        value = self.np_random.choice(keys)\r\n",
    "        return value\r\n",
    "\r\n",
    "    def _update_probability_matrix(self, predicted_pos):\r\n",
    "        newrow, newcol = predicted_pos\r\n",
    "        newstate = self._to_s(newrow, newcol)\r\n",
    "        newletter = self.desc[newrow, newcol]\r\n",
    "        done = bytes(newletter) in [self.l_goal]\r\n",
    "        #done = bytes(newletter) in [self.l_goal, self.l_wall]\r\n",
    "        #reward = float(newletter == self.l_goal \r\n",
    "        #    and newcol < (self.ncol-1)\r\n",
    "        #    and self.desc[newrow, newcol+1] == self.l_object)        \r\n",
    "        rewards = {\r\n",
    "            self.l_goal: 100.0,\r\n",
    "            self.l_wall: -100.0,\r\n",
    "        }\r\n",
    "        reward = rewards.get(newletter, -1)\r\n",
    "        return newstate, reward, done\r\n",
    "\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        super().reset()\r\n",
    "        self.steps = [self.s] # para renderizar o caminho\r\n",
    "\r\n",
    "    def step(self, a):\r\n",
    "        result = super().step(a)\r\n",
    "        self.steps.append(self.s)\r\n",
    "        return result\r\n",
    "\r\n",
    "    def render(self, mode=\"human\"):\r\n",
    "        outfile = StringIO() if mode == \"ansi\" else sys.stdout\r\n",
    "\r\n",
    "        desc = self.desc.tolist()\r\n",
    "        desc2 = desc[:]\r\n",
    "        colors = {\r\n",
    "            self.l_goal: 'blue',\r\n",
    "            self.l_agent: 'red',\r\n",
    "            self.l_wall: 'white',\r\n",
    "            self.l_object: 'magenta',\r\n",
    "        }\r\n",
    "        for row in range(len(desc)):\r\n",
    "            for col in range(len(desc[row])):\r\n",
    "                sb = desc[row][col]\r\n",
    "                color = colors.get(sb)\r\n",
    "                ss = sb.decode('utf-8')\r\n",
    "                desc2[row][col] = gym.utils.colorize(ss, color, highlight=True) if color else ss\r\n",
    "\r\n",
    "        for _s in self.steps:\r\n",
    "            row, col = self._s_to_row_col(_s)\r\n",
    "            color = colors.get(desc[row][col])\r\n",
    "            walked = \"~\"\r\n",
    "            desc2[row][col] = gym.utils.colorize(walked, color, highlight=True) if color else walked\r\n",
    "        desc2[row][col] = gym.utils.colorize(self.l_agent.decode('utf-8'), colors.get(self.l_agent), highlight=True)\r\n",
    "\r\n",
    "        if self.lastaction is not None:\r\n",
    "            outfile.write(\r\n",
    "                \"  ({})\\n\".format([\"Left\", \"Down\", \"Right\", \"Up\"][self.lastaction])\r\n",
    "            )\r\n",
    "        else:\r\n",
    "            outfile.write(\"\\n\")\r\n",
    "        outfile.write(\"\\n\".join(\" \".join(line) for line in desc2) + \"\\n\")\r\n",
    "\r\n",
    "        if mode != \"human\":\r\n",
    "            with closing(outfile):\r\n",
    "                return outfile.getvalue()\r\n",
    "\r\n",
    "t1_map = [\r\n",
    "    \"__$$$__\",\r\n",
    "    \"___#___\",\r\n",
    "    \"___+___\",\r\n",
    "    \"_______\",\r\n",
    "    \"##_####\",\r\n",
    "    \"o_____#\"\r\n",
    "]\r\n",
    "\r\n",
    "t1_env = ObjectTransportEnv(t1_map)\r\n",
    "t1_env.reset()\r\n",
    "t1_env.render()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "_ _ \u001b[44m$\u001b[0m \u001b[44m$\u001b[0m \u001b[44m$\u001b[0m _ _\n",
      "_ _ _ \u001b[47m#\u001b[0m _ _ _\n",
      "_ _ _ \u001b[45m+\u001b[0m _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "\u001b[47m#\u001b[0m \u001b[47m#\u001b[0m _ \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m\n",
      "\u001b[41mo\u001b[0m _ _ _ _ _ \u001b[47m#\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "source": [
    "def Qlearning(environment, num_episodes=100, alpha=0.3, gamma=0.9, epsilon=1.0, decay_epsilon=0.1, max_epsilon=1.0, min_epsilon=0.01):\r\n",
    "  \r\n",
    "  # initializing the Q-table\r\n",
    "  Q = np.zeros((environment.observation_space.n, environment.action_space.n))\r\n",
    "  \r\n",
    "  # additional lists to keep track of reward and epsilon values\r\n",
    "  rewards = []\r\n",
    "  epsilons = []\r\n",
    "  last_accumulated_reward = -999999\r\n",
    "\r\n",
    "  # episodes\r\n",
    "  for episode in range(num_episodes):\r\n",
    "      \r\n",
    "      # reset the environment to start a new episode\r\n",
    "      state = environment.reset()\r\n",
    "\r\n",
    "      # reward accumulated along episode\r\n",
    "      accumulated_reward = 0\r\n",
    "      \r\n",
    "      # steps within current episode\r\n",
    "      for step in range(100):\r\n",
    "          action = None\r\n",
    "\r\n",
    "          # epsilon-greedy action selection\r\n",
    "          # exploit with probability 1-epsilon\r\n",
    "          if np.random.uniform(0, 1) > epsilon:\r\n",
    "              action = np.argmax(Q[state,:])\r\n",
    "              if not environment.is_valid_current_action(action):\r\n",
    "                action = None\r\n",
    "              #else:\r\n",
    "                #print('action1', ['left','up','right','down'][action], action, Q[state,:])\r\n",
    "          # explore with probability epsilon\r\n",
    "          if not action:\r\n",
    "              #action = environment.action_space.sample()\r\n",
    "              action = environment.sample_from_available_current_actions()\r\n",
    "              #print('action2', ['left','up','right','down'][action], action, environment.P[environment.s])\r\n",
    "\r\n",
    "          # perform the action and observe the new state and corresponding reward\r\n",
    "          new_state, reward, done, info = environment.step(action)\r\n",
    "          #print('state', new_state, 'reward', reward, 'done', done)\r\n",
    "\r\n",
    "          # update the Q-table\r\n",
    "          Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])\r\n",
    "          \r\n",
    "          # update the accumulated reward\r\n",
    "          accumulated_reward += reward\r\n",
    "          #print(Q)\r\n",
    "\r\n",
    "          # update the current state\r\n",
    "          state = new_state\r\n",
    "\r\n",
    "          # end the episode when it is done\r\n",
    "          if done == True:\r\n",
    "              break\r\n",
    "      \r\n",
    "      # decay exploration rate to ensure that the agent exploits more as it becomes experienced\r\n",
    "      epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_epsilon*episode)\r\n",
    "      \r\n",
    "      # update the lists of rewards and epsilons\r\n",
    "      rewards.append(accumulated_reward)\r\n",
    "      epsilons.append(epsilon)\r\n",
    "\r\n",
    "      # Teste pra mostrar cada caminho melhor\r\n",
    "      #if accumulated_reward > last_accumulated_reward:\r\n",
    "      #  environment.render()\r\n",
    "      #  print('>' * 10, episode, last_accumulated_reward, '->', accumulated_reward)\r\n",
    "      #  last_accumulated_reward = accumulated_reward \r\n",
    "      #break\r\n",
    "\r\n",
    "\r\n",
    "  # render the environment\r\n",
    "  environment.render()\r\n",
    "  print(accumulated_reward)\r\n",
    "    \r\n",
    "  # return the list of accumulated reward along episodes\r\n",
    "  return rewards"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "source": [
    "#num_episodes=100\r\n",
    "num_episodes=10000\r\n",
    "alpha=0.3\r\n",
    "gamma=0.9\r\n",
    "epsilon=1.0\r\n",
    "decay_epsilon=0.1\r\n",
    "\r\n",
    "# run Q-learning\r\n",
    "rewards = Qlearning(t1_env, num_episodes, alpha, gamma, epsilon, decay_epsilon)\r\n",
    "\r\n",
    "# print results\r\n",
    "print (\"Average reward (all episodes): \" + str(sum(rewards)/num_episodes))\r\n",
    "print (\"Average reward (last 10 episodes): \" + str(sum(rewards[-10:])/10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (Down)\n",
      "_ _ \u001b[44m$\u001b[0m \u001b[44m$\u001b[0m \u001b[41mo\u001b[0m _ _\n",
      "_ _ _ \u001b[47m#\u001b[0m ~ _ _\n",
      "_ _ _ ~ ~ _ _\n",
      "_ _ ~ ~ _ _ _\n",
      "\u001b[47m#\u001b[0m \u001b[47m#\u001b[0m ~ \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m\n",
      "~ ~ ~ _ _ _ \u001b[47m#\u001b[0m\n",
      "92.0\n",
      "Average reward (all episodes): 13.2495\n",
      "Average reward (last 10 episodes): 72.2\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('venv': virtualenv)"
  },
  "interpreter": {
   "hash": "4337733d7c4f8242ca10155413965bee5981b415db72db4cb080555abb5e32aa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}