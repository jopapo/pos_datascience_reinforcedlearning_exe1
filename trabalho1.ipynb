{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Trabalho Prático I - Problema de Transporte de Objeto\r\n",
    "\r\n",
    "Descrição na [ementa](enunciado_trabalho_I_v2.pdf).\r\n",
    "\r\n",
    "Neste cenário, um agente deve percorrer a grade 7x6, encontrar o objeto e transportá-lo até na base. Essa tarefa deve ser executada na menor quantidade de passos detempo possível. O agente não possui nenhum conhecimento prévio sobre o ambiente, o qual possui paredes, as quais ele não pode transpor. O agente também não possui conhecimento prévio sobre a localização do objeto. A localização inicial do agente, disposição das paredes e objeto são sempre fixas, conforme indicado na ilustração. A cada passo de tempo, o agente pode executar os seguintes movimentos na grade:\r\n",
    "- mover para cima, baixo, esquerda ou direita;\r\n",
    "- permanecer na mesma célula;\r\n",
    "\r\n",
    "Este cenário apresenta algumas restrições de movimentação:\r\n",
    "- O agente pode realizar apenas uma movimentação por passo de tempo;\r\n",
    "- Se o agente escolher se mover para uma célula que não está vazia, seja por conta de uma parede ou objeto, ele não se move, i.e., permanece na mesma célula;\r\n",
    "- Qualquer tentativa de locomoção para além da grade, resultará na não movimentação do agente;\r\n",
    "- O objeto sé pode ser agarrado pela sua esquerda ou direita;\r\n",
    "- Quando o agente ́e posicionado à direita ou esquerda do objeto, o objeto ée agarrado automaticamente;\r\n",
    "- Uma vez agarrado o objeto, o agente não pode soltá-lo;\r\n",
    "- O agente, quando agarrado ao objeto, só consegue se mover para uma nova célula desde que não haja nenhuma restrição de movimentação para o agente e objeto;\r\n",
    "\r\n",
    "O episódio ée concluído automaticamente quando o objeto entra na base ou se atingir um número máximo de passos de tempo sem resolver a tarefa. Em ambos os casos, um novo episódio ́é iniciado, com o agente e objeto situados conforme abaixo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Libs necessária\r\n",
    "import gym\r\n",
    "from gym.envs.toy_text import discrete\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "#import random\r\n",
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Estratégia\r\n",
    "\r\n",
    "Pensei em algumas estratégias: Criar dois agentes, um que vai até o objeto e outro que vai com o objeto até o objetivo; Ou criar um agente que possui pontuação somente se chegar no objetivo com o objeto, porém, aqui precisamos de um modelo onde as transições não podem ser pré-definidas. Sendo assim o algoritmo foi alterado para identificar as transições possíveis de acordo com a posição do agente e sua interação com o ambiente (se tem ou não o objeto). Outro detalhe é o custo de andar (para objetivar o menor camino), pra isso foi importante colocar que um passo, mesmo que válido, aplica uma \"punição\" de -1.\r\n",
    "\r\n",
    "Iniciamente usei o DiscreteEnv do Gym para isso. Mas foi necessário ampliar para busca das transições de forma dinâmica. Ainda extendi Env e reimplementei as partes do DiscreteEnv que me interessavam.\r\n",
    "\r\n",
    "Outro detalhe da estratégia foram as paredes. Imaginei que poderia remover a opção de movimento válido nas paredes. Acredito que funciona pois diminui o espaço de transições possíveis. Porém, soa como \"trapassa\". Mas como está no enunciado, optei por seguir dessa forma. Pensei também em uma medida mais lógica que prática: objeto também custa. Então, segurar o objeto começa a custar tudo conforme o terreno onde está o objeto. Porém, não houve impacto na eficiência do algoritmo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# Montando o cenário \r\n",
    "# Exemplo 1: https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e\r\n",
    "# Exemplo 2: https://github.com/caburu/gym-cliffwalking/blob/master/gym_cliffwalking/envs/cliffwalking_env.py\r\n",
    "# Exemplo 3: https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\r\n",
    "\r\n",
    "class ObjectTransportEnv(discrete.Env):\r\n",
    "    \"\"\"\r\n",
    "    O mapa é descrito com:\r\n",
    "    _ : caminho livre\r\n",
    "    o : posição inicial do agente\r\n",
    "    + : objeto a ser transportado\r\n",
    "    $ : objetivo\r\n",
    "    # : bloqueio\r\n",
    "    O episódio termina chegar no objetivo.\r\n",
    "    Sua recompensa é 1 se pegar o objeto e 1 se chegar ao objetivo com o objeto.\r\n",
    "    Fora do mapa não é acessível. Mais regras na ementa acima.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    metadata = {\"render.modes\": [\"human\", \"ansi\"]}\r\n",
    "    l_free_path = b'_'\r\n",
    "    l_agent = b'o'\r\n",
    "    l_object = b'+'\r\n",
    "    l_goal = b'$'\r\n",
    "    l_wall = b'#'\r\n",
    "    actions_movements = [\r\n",
    "        (0, -1) # left\r\n",
    "        ,(-1, 0) # up\r\n",
    "        ,(0, 1) # rigth\r\n",
    "        ,(1, 0) # down\r\n",
    "    ]\r\n",
    "    agent_steps = []\r\n",
    "\r\n",
    "    def __init__(self, desc):\r\n",
    "        self.desc = np.asarray(desc, dtype=\"c\")\r\n",
    "        self.nrow, self.ncol = self.desc.shape\r\n",
    "        self.reward_range = (-100.0, 100.0)\r\n",
    "\r\n",
    "        # 4 ações (cima, baixo, esquerda, direita)\r\n",
    "        nA = len(self.actions_movements)\r\n",
    "        # Espaço é o tamanho do mapa\r\n",
    "        nS = self.nrow * self.ncol\r\n",
    "\r\n",
    "        # estado inicial (se tiver mais que um - escolhe um aleatório no reset)\r\n",
    "        isd = np.array(self.desc == self.l_agent).astype(\"float64\").ravel()\r\n",
    "        isd /= isd.sum()\r\n",
    "\r\n",
    "        # lista com transições (probability, nextstate, reward, done)\r\n",
    "        # P = {s: {} for s in range(nS)}        \r\n",
    "        # for row in range(self.nrow):\r\n",
    "        #     for col in range(self.ncol):\r\n",
    "        #         s = self._to_s(row, col)\r\n",
    "        #         # movimento protegido para não ir pra fora da lista\r\n",
    "        #         #for action, predicted_pos in self._possible_actions(row, col):\r\n",
    "        #         for action, predicted_pos in self._possible_actions(row, col):\r\n",
    "        #             P[s][action] = [(1.0, *self._update_probability_matrix(predicted_pos))]\r\n",
    "\r\n",
    "        #super(ObjectTransportEnv, self).__init__(nS, nA, P, isd)\r\n",
    "        \r\n",
    "        #self.P = P\r\n",
    "        self.isd = isd\r\n",
    "        self.nS = nS\r\n",
    "        self.nA = nA\r\n",
    "\r\n",
    "        self.action_space = gym.spaces.Discrete(self.nA)\r\n",
    "        # Não entendi pra que é usado no modelo. Acredito que é pra quando as posições iniciais são aleatórias.\r\n",
    "        self.observation_space = gym.spaces.Discrete(self.nS)\r\n",
    "\r\n",
    "        self.seed()\r\n",
    "        self.s = discrete.categorical_sample(self.isd, self.np_random)\r\n",
    "        self.os = None\r\n",
    "\r\n",
    "    def seed(self, seed=None):\r\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\r\n",
    "        return [seed]\r\n",
    "\r\n",
    "    def move(self, pos, action):\r\n",
    "        incrow, inccol = action\r\n",
    "        row, col = pos\r\n",
    "        newpos = (row + incrow, col + inccol)\r\n",
    "        return newpos\r\n",
    "\r\n",
    "    def rowcol_to_state(self, row, col):\r\n",
    "        return row * self.ncol + col\r\n",
    "\r\n",
    "    def pos_to_state(self, pos):\r\n",
    "        row, col = pos\r\n",
    "        return self.rowcol_to_state(row, col)\r\n",
    "\r\n",
    "    def state_to_rowcol(self, s = None):\r\n",
    "        _s = s or self.s\r\n",
    "        return _s // self.ncol, _s % self.ncol\r\n",
    "\r\n",
    "    def is_inside(self, pos):\r\n",
    "        row, col = pos\r\n",
    "        inside = row >= 0 and col >= 0 and row < self.nrow and col < self.ncol\r\n",
    "        return inside\r\n",
    "\r\n",
    "    def is_valid_pos(self, pos):\r\n",
    "        inside = self.is_inside(pos)\r\n",
    "        walkable = False\r\n",
    "        if inside:\r\n",
    "            row, col = pos\r\n",
    "            # Se está segurando, o ponto original do objeto não é mais o mesmo\r\n",
    "            valids = [self.l_free_path, self.l_goal, (self.l_object if self.os else b'')]\r\n",
    "            walkable = self.desc[row, col] in valids\r\n",
    "        return inside and walkable\r\n",
    "\r\n",
    "    def is_valid_current_action(self, action):\r\n",
    "        transitions = self.possible_actions()\r\n",
    "        return action in transitions\r\n",
    "\r\n",
    "    def sample_from_available_current_actions(self):\r\n",
    "        transitions = self.possible_actions()\r\n",
    "        keys = list(transitions.keys())\r\n",
    "        value = self.np_random.choice(keys)\r\n",
    "        return value\r\n",
    "\r\n",
    "    def action_possible_transitions(self, pos, action):\r\n",
    "        newpos = self.move(pos, action)\r\n",
    "        newpos_obj = None\r\n",
    "        objpos = self.state_to_rowcol(self.os) if self.os else None\r\n",
    "        if not self.is_valid_pos(newpos):\r\n",
    "            return None # Se estiver fora do mapa já cai for\r\n",
    "        if objpos:\r\n",
    "            newpos_obj = self.move(objpos, action) # Objeto move na mesma direção do agente\r\n",
    "            if not self.is_valid_pos(newpos_obj):\r\n",
    "                return None # Se o objeto estiver fora do mapa ou na parede, também não é um caminho válido\r\n",
    "        else:\r\n",
    "            # Checa se começou a segurar o objeto à esquerda ou direita\r\n",
    "            for action_check in [self.actions_movements[0], self.actions_movements[2]]:\r\n",
    "                newpos_obj_temp = self.move(newpos, action_check)\r\n",
    "                if self.is_inside(newpos_obj_temp) and self.desc[newpos_obj_temp[0], newpos_obj_temp[1]] == self.l_object:\r\n",
    "                    newpos_obj = newpos_obj_temp\r\n",
    "\r\n",
    "        #print(pos, newpos, objpos, newpos_obj)\r\n",
    "\r\n",
    "        newrow, newcol = newpos\r\n",
    "        newletter = self.desc[newrow, newcol]\r\n",
    "        \r\n",
    "        rewards_points = {\r\n",
    "            self.l_goal: 100.0,\r\n",
    "            self.l_object: -1, # mesmo que freepath. Ele só passa se estiver segurando o objeto.\r\n",
    "            self.l_free_path: -1,\r\n",
    "        }\r\n",
    "\r\n",
    "        reward = rewards_points.get(self.desc[newrow, newcol])\r\n",
    "        if reward == None:\r\n",
    "            raise Exception(\"Achou um caminho válido sem recompensa. Valide a regra: \", pos, newpos, newletter, reward, newpos_obj)\r\n",
    "\r\n",
    "        done = newpos_obj and bytes(newletter) in [self.l_goal]\r\n",
    "\r\n",
    "        prob = 1.0 # Tudo é 100% de probabilidade pq não tem esse tipo de medição no nosso modelo\r\n",
    "\r\n",
    "        newstate = self.pos_to_state(newpos)\r\n",
    "        newobjectstate = (self.pos_to_state(newpos_obj) if newpos_obj else None)\r\n",
    "        \r\n",
    "        return prob, newstate, reward, done, newobjectstate\r\n",
    "\r\n",
    "    def possible_actions(self, pos = None):\r\n",
    "        _pos = pos or self.state_to_rowcol(self.s)\r\n",
    "        _p = {}\r\n",
    "        for index, action in enumerate(self.actions_movements):\r\n",
    "            result = self.action_possible_transitions(_pos, action)\r\n",
    "            if result:\r\n",
    "                _p[index] = result\r\n",
    "        #print(_p)\r\n",
    "        return _p\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.s = discrete.categorical_sample(self.isd, self.np_random)\r\n",
    "        self.os = None\r\n",
    "        self.agent_steps = [] # para renderizar o caminho\r\n",
    "        return int(self.s)\r\n",
    "\r\n",
    "    def step(self, action_index):\r\n",
    "        action = self.actions_movements[action_index]\r\n",
    "        pos = self.state_to_rowcol()\r\n",
    "        transitions = [self.action_possible_transitions(pos, action)]\r\n",
    "        i = discrete.categorical_sample([t[0] for t in transitions], self.np_random)\r\n",
    "        p, s, r, d, os = transitions[i]\r\n",
    "        self.s = s\r\n",
    "        self.os = os\r\n",
    "        self.agent_steps.append(self.s)\r\n",
    "        return (int(s), r, d, {\"prob\": p})\r\n",
    "\r\n",
    "    def render(self, mode=\"human\"):\r\n",
    "        outfile = StringIO() if mode == \"ansi\" else sys.stdout\r\n",
    "\r\n",
    "        desc = self.desc.tolist()\r\n",
    "        desc2 = desc[:]\r\n",
    "        colors = {\r\n",
    "            self.l_goal: 'blue',\r\n",
    "            self.l_agent: 'red',\r\n",
    "            self.l_wall: 'white',\r\n",
    "            self.l_object: 'magenta',\r\n",
    "        }\r\n",
    "        \r\n",
    "        for row in range(len(desc)):\r\n",
    "            for col in range(len(desc[row])):\r\n",
    "                sb = desc[row][col]\r\n",
    "                color = colors.get(sb)\r\n",
    "                ss = sb.decode('utf-8')\r\n",
    "                desc2[row][col] = gym.utils.colorize(ss, color, highlight=True) if color else ss\r\n",
    "\r\n",
    "        if self.agent_steps:\r\n",
    "            for _s in self.agent_steps:\r\n",
    "                row, col = self.state_to_rowcol(_s)\r\n",
    "                color = colors.get(desc[row][col])\r\n",
    "                walked = \"~\"\r\n",
    "                desc2[row][col] = gym.utils.colorize(walked, color, highlight=True) if color else walked\r\n",
    "            text = self.l_agent.decode('utf-8')\r\n",
    "            if self.os:\r\n",
    "                text += ' ' + self.l_object.decode('utf-8')\r\n",
    "            desc2[row][col] = gym.utils.colorize(text, colors.get(self.l_agent), highlight=True)\r\n",
    "\r\n",
    "        outfile.write(\"\\n\".join(\" \".join(line) for line in desc2) + \"\\n\")\r\n",
    "\r\n",
    "        if mode != \"human\":\r\n",
    "            with closing(outfile):\r\n",
    "                return outfile.getvalue()\r\n",
    "\r\n",
    "t1_map = [\r\n",
    "    \"__$$$__\",\r\n",
    "    \"___#___\",\r\n",
    "    \"___+___\",\r\n",
    "    \"_______\",\r\n",
    "    \"##_####\",\r\n",
    "    \"o_____#\"\r\n",
    "]\r\n",
    "\r\n",
    "t1_env = ObjectTransportEnv(t1_map)\r\n",
    "t1_env.reset()\r\n",
    "t1_env.render()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_ _ \u001b[44m$\u001b[0m \u001b[44m$\u001b[0m \u001b[44m$\u001b[0m _ _\n",
      "_ _ _ \u001b[47m#\u001b[0m _ _ _\n",
      "_ _ _ \u001b[45m+\u001b[0m _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "\u001b[47m#\u001b[0m \u001b[47m#\u001b[0m _ \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m\n",
      "\u001b[41mo\u001b[0m _ _ _ _ _ \u001b[47m#\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "def Qlearning(environment, num_episodes=100, alpha=0.3, gamma=0.9, epsilon=1.0, decay_epsilon=0.1, max_epsilon=1.0, min_epsilon=0.01):\r\n",
    "  \r\n",
    "  # initializing the Q-table\r\n",
    "  Q = np.zeros((environment.observation_space.n, environment.action_space.n))\r\n",
    "  \r\n",
    "  # additional lists to keep track of reward and epsilon values\r\n",
    "  rewards = []\r\n",
    "  epsilons = []\r\n",
    "  last_accumulated_reward = -999999\r\n",
    "\r\n",
    "  # episodes\r\n",
    "  for episode in range(num_episodes):\r\n",
    "      \r\n",
    "      # reset the environment to start a new episode\r\n",
    "      state = environment.reset()\r\n",
    "\r\n",
    "      # reward accumulated along episode\r\n",
    "      accumulated_reward = 0\r\n",
    "      \r\n",
    "      # steps within current episode\r\n",
    "      for step in range(100):\r\n",
    "          action = None\r\n",
    "\r\n",
    "          # epsilon-greedy action selection\r\n",
    "          # exploit with probability 1-epsilon\r\n",
    "          if np.random.uniform(0, 1) > epsilon:\r\n",
    "              action = np.argmax(Q[state,:])\r\n",
    "              if not environment.is_valid_current_action(action):\r\n",
    "                action = None\r\n",
    "              #else:\r\n",
    "              #  print('action1', ['left','up','right','down'][action], action, Q[state,:])\r\n",
    "          # explore with probability epsilon\r\n",
    "          if not action:\r\n",
    "              #action = environment.action_space.sample()\r\n",
    "              action = environment.sample_from_available_current_actions()\r\n",
    "              #print('action2', ['left','up','right','down'][action], action)\r\n",
    "\r\n",
    "          # perform the action and observe the new state and corresponding reward\r\n",
    "          new_state, reward, done, info = environment.step(action)\r\n",
    "          #print('state', new_state, 'reward', reward, 'done', done)\r\n",
    "\r\n",
    "          # update the Q-table\r\n",
    "          Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])\r\n",
    "          \r\n",
    "          # update the accumulated reward\r\n",
    "          accumulated_reward += reward\r\n",
    "          #print(Q)\r\n",
    "\r\n",
    "          # update the current state\r\n",
    "          state = new_state\r\n",
    "\r\n",
    "          # end the episode when it is done\r\n",
    "          if done == True:\r\n",
    "              break\r\n",
    "      \r\n",
    "      # decay exploration rate to ensure that the agent exploits more as it becomes experienced\r\n",
    "      epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_epsilon*episode)\r\n",
    "      \r\n",
    "      # update the lists of rewards and epsilons\r\n",
    "      rewards.append(accumulated_reward)\r\n",
    "      epsilons.append(epsilon)\r\n",
    "\r\n",
    "      # Teste pra mostrar cada caminho melhor\r\n",
    "      # if accumulated_reward > last_accumulated_reward:\r\n",
    "      #   environment.render()\r\n",
    "      #   print('>' * 10, episode, last_accumulated_reward, '->', accumulated_reward)\r\n",
    "      #   last_accumulated_reward = accumulated_reward \r\n",
    "      \r\n",
    "      #break\r\n",
    "\r\n",
    "\r\n",
    "  # render the environment\r\n",
    "  environment.render()\r\n",
    "  print(accumulated_reward)\r\n",
    "    \r\n",
    "  # return the list of accumulated reward along episodes\r\n",
    "  return rewards"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "%%time\r\n",
    "\r\n",
    "num_episodes=1000\r\n",
    "alpha=0.3 # Peso para atualização do novo caminho descoberto\r\n",
    "gamma=0.9\r\n",
    "epsilon=1.0\r\n",
    "decay_epsilon=0.1\r\n",
    "\r\n",
    "# run Q-learning\r\n",
    "rewards = Qlearning(t1_env, num_episodes, alpha, gamma, epsilon, decay_epsilon)\r\n",
    "\r\n",
    "# print results\r\n",
    "print (\"Average reward (all episodes): \" + str(sum(rewards)/num_episodes))\r\n",
    "print (\"Average reward (last 10 episodes): \" + str(sum(rewards[-10:])/10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_ _ \u001b[44m$\u001b[0m \u001b[44m$\u001b[0m \u001b[41mo +\u001b[0m _ _\n",
      "_ _ _ \u001b[47m#\u001b[0m ~ _ _\n",
      "_ _ ~ ~ ~ _ _\n",
      "_ _ ~ _ _ _ _\n",
      "\u001b[47m#\u001b[0m \u001b[47m#\u001b[0m ~ \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m \u001b[47m#\u001b[0m\n",
      "\u001b[41mo\u001b[0m ~ ~ _ _ _ \u001b[47m#\u001b[0m\n",
      "92.0\n",
      "Average reward (all episodes): 92.497\n",
      "Average reward (last 10 episodes): 91.8\n",
      "Wall time: 651 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Solução\r\n",
    "\r\n",
    "A solução me surpreendeu.\r\n",
    "\r\n",
    "Na minha cabeça o melhor caminho seria pela esquerda da parede em frente ao objetivo. Mas pra fazer pela esquerda seria necessário dois passos a mais. O algoritmo conseguiu achar o caminho pela direita da parede pois no meu exercício mental, o objeto deveria chegar ao objetivo também. Porém, isso não está nas regras.\r\n",
    "\r\n",
    "# Observações\r\n",
    "\r\n",
    "A solução implementada permite um mapa flexível com vários objetos e se entenderia dinamicamente."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('venv': virtualenv)"
  },
  "interpreter": {
   "hash": "4337733d7c4f8242ca10155413965bee5981b415db72db4cb080555abb5e32aa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}