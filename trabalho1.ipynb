{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Trabalho Prático I - Problema de Transporte de Objeto\r\n",
    "\r\n",
    "Descrição na [ementa](enunciado_trabalho_I_v2.pdf).\r\n",
    "\r\n",
    "Neste cenário, um agente deve percorrer a grade 7x6, encontrar o objeto e transportá-lo até na base. Essa tarefa deve ser executada na menor quantidade de passos detempo possível. O agente não possui nenhum conhecimento prévio sobre o ambiente, o qual possui paredes, as quais ele não pode transpor. O agente também não possui conhecimento prévio sobre a localização do objeto. A localização inicial do agente, disposição das paredes e objeto são sempre fixas, conforme indicado na ilustração. A cada passo de tempo, o agente pode executar os seguintes movimentos na grade:\r\n",
    "- mover para cima, baixo, esquerda ou direita;\r\n",
    "- permanecer na mesma célula;\r\n",
    "\r\n",
    "Este cenário apresenta algumas restrições de movimentação:\r\n",
    "- O agente pode realizar apenas uma movimentação por passo de tempo;\r\n",
    "- Se o agente escolher se mover para uma célula que não está vazia, seja por conta de uma parede ou objeto, ele não se move, i.e., permanece na mesma célula;\r\n",
    "- Qualquer tentativa de locomoção para além da grade, resultará na não movimentação do agente;\r\n",
    "- O objeto sé pode ser agarrado pela sua esquerda ou direita;\r\n",
    "- Quando o agente ́e posicionado à direita ou esquerda do objeto, o objeto ée agarrado automaticamente;\r\n",
    "- Uma vez agarrado o objeto, o agente não pode soltá-lo;\r\n",
    "- O agente, quando agarrado ao objeto, só consegue se mover para uma nova célula desde que não haja nenhuma restrição de movimentação para o agente e objeto;\r\n",
    "\r\n",
    "O episódio ée concluído automaticamente quando o objeto entra na base ou se atingir um número máximo de passos de tempo sem resolver a tarefa. Em ambos os casos, um novo episódio ́é iniciado, com o agente e objeto situados conforme abaixo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Libs necessária\r\n",
    "import gym\r\n",
    "from gym.envs.toy_text import discrete\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "#import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# Montando o cenário \r\n",
    "# Exemplo 1: https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e\r\n",
    "# Exemplo 2: https://github.com/caburu/gym-cliffwalking/blob/master/gym_cliffwalking/envs/cliffwalking_env.py\r\n",
    "# Exemplo 3: https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\r\n",
    "\r\n",
    "class ObjectTransportEnv(discrete.DiscreteEnv):\r\n",
    "    \"\"\"\r\n",
    "    O mapa é descrito com:\r\n",
    "    _ : caminho livre\r\n",
    "    o : posição inicial do agente\r\n",
    "    + : objeto a ser transportado\r\n",
    "    $ : objetivo\r\n",
    "    # : bloqueio\r\n",
    "    O episódio termina chegar no objetivo.\r\n",
    "    Sua recompensa é 1 se pegar o objeto e 1 se chegar ao objetivo com o objeto.\r\n",
    "    Fora do mapa não é acessível. Mais regras na ementa acima.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    metadata = {\"render.modes\": [\"human\", \"ansi\"]}\r\n",
    "    l_free_path = b'_'\r\n",
    "    l_agent = b'o'\r\n",
    "    l_object = b'+'\r\n",
    "    l_goal = b'$'\r\n",
    "    l_wall = b'#'\r\n",
    "    actions_movements = [\r\n",
    "        (0, -1) # left\r\n",
    "        ,(-1, 0) # up\r\n",
    "        ,(0, 1) # rigth\r\n",
    "        ,(1, 0) # down\r\n",
    "    ]\r\n",
    "\r\n",
    "    def _inc(self, row, col, action):\r\n",
    "        inc_row, inc_col = action\r\n",
    "        col = col + inc_col\r\n",
    "        row = row + inc_row\r\n",
    "        return (row, col)\r\n",
    "\r\n",
    "    def _update_probability_matrix(self, predicted_pos):\r\n",
    "        newrow, newcol = predicted_pos\r\n",
    "        newstate = self._to_s(newrow, newcol)\r\n",
    "        newletter = self.desc[newrow, newcol]\r\n",
    "        done = bytes(newletter) in self.l_goal\r\n",
    "        reward = float(newletter == self.l_goal \r\n",
    "            and newcol < (self.ncol-1)\r\n",
    "            and self.desc[newrow, newcol+1] == self.l_object)\r\n",
    "        return newstate, reward, done\r\n",
    "\r\n",
    "    def _to_s(self, row, col):\r\n",
    "        return row * self.ncol + col\r\n",
    "\r\n",
    "    def _possible_actions(self, row, col):\r\n",
    "        for index in range(len(self.actions_movements)):\r\n",
    "            newpos = self._inc(row, col, self.actions_movements[index])\r\n",
    "            if self._is_valid_pos(newpos):\r\n",
    "                yield index, newpos\r\n",
    "    \r\n",
    "    def _is_valid_pos(self, pos):\r\n",
    "        row, col = pos\r\n",
    "        return row >= 0 and col >= 0 and row < self.nrow and col < self.ncol\r\n",
    "\r\n",
    "    def __init__(self, desc):\r\n",
    "        self.desc = np.asarray(desc, dtype=\"c\")\r\n",
    "        self.nrow, self.ncol = self.desc.shape\r\n",
    "        self.reward_range = (0, 1)\r\n",
    "\r\n",
    "        # 4 ações (cima, baixo, esquerda, direita)\r\n",
    "        nA = len(self.actions_movements)\r\n",
    "        # Espaço é o tamanho do mapa\r\n",
    "        nS = self.nrow * self.ncol\r\n",
    "\r\n",
    "        # estado inicial\r\n",
    "        isd = np.array(self.desc == self.l_agent).astype(\"float64\").ravel()\r\n",
    "        isd /= isd.sum()\r\n",
    "\r\n",
    "        # lista com transições (probability, nextstate, reward, done)\r\n",
    "        P = {s: {a: [(0.0, s, 0.0, False)] for a in range(nA)} for s in range(nS)}\r\n",
    "        \r\n",
    "        for row in range(self.nrow):\r\n",
    "            for col in range(self.ncol):\r\n",
    "                s = self._to_s(row, col)\r\n",
    "                # movimento protegido para não ir pra fora da lista\r\n",
    "                #for action, predicted_pos in self._possible_actions(row, col):\r\n",
    "                for action, predicted_pos in self._possible_actions(row, col):\r\n",
    "                    li = P[s][action]\r\n",
    "                    li.append((1.0, *self._update_probability_matrix(predicted_pos)))\r\n",
    "\r\n",
    "        super(ObjectTransportEnv, self).__init__(nS, nA, P, isd)\r\n",
    "\r\n",
    "    def render(self, mode=\"human\"):\r\n",
    "        outfile = StringIO() if mode == \"ansi\" else sys.stdout\r\n",
    "\r\n",
    "        desc = self.desc.tolist()\r\n",
    "        colors = {\r\n",
    "            self.l_goal: 'blue',\r\n",
    "            self.l_agent: 'red',\r\n",
    "            self.l_wall: 'white',\r\n",
    "            self.l_object: 'magenta',\r\n",
    "        }\r\n",
    "        for row in range(len(desc)):\r\n",
    "            for col in range(len(desc[row])):\r\n",
    "                sb = desc[row][col]\r\n",
    "                color = colors.get(sb)\r\n",
    "                ss = sb.decode('utf-8')\r\n",
    "                desc[row][col] = gym.utils.colorize(ss, color, highlight=True) if color else ss\r\n",
    "\r\n",
    "        if self.lastaction is not None:\r\n",
    "            outfile.write(\r\n",
    "                \"  ({})\\n\".format([\"Left\", \"Down\", \"Right\", \"Up\"][self.lastaction])\r\n",
    "            )\r\n",
    "        else:\r\n",
    "            outfile.write(\"\\n\")\r\n",
    "        outfile.write(\"\\n\".join(\"\".join(line) for line in desc) + \"\\n\")\r\n",
    "\r\n",
    "        if mode != \"human\":\r\n",
    "            with closing(outfile):\r\n",
    "                return outfile.getvalue()\r\n",
    "\r\n",
    "t1_map = [\r\n",
    "    \"__$$$__\",\r\n",
    "    \"___#___\",\r\n",
    "    \"___+___\",\r\n",
    "    \"_______\",\r\n",
    "    \"##_####\",\r\n",
    "    \"o_____#\"\r\n",
    "]\r\n",
    "\r\n",
    "t1_env = ObjectTransportEnv(t1_map)\r\n",
    "t1_env.reset()\r\n",
    "t1_env.render()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: {0: [(0.0, 0, 0.0, False)], 1: [(0.0, 0, 0.0, False)], 2: [(0.0, 0, 0.0, False), (1.0, 1, 0.0, False)], 3: [(0.0, 0, 0.0, False), (1.0, 7, 0.0, False)]}, 1: {0: [(0.0, 1, 0.0, False), (1.0, 0, 0.0, False)], 1: [(0.0, 1, 0.0, False)], 2: [(0.0, 1, 0.0, False), (1.0, 2, 0.0, True)], 3: [(0.0, 1, 0.0, False), (1.0, 8, 0.0, False)]}, 2: {0: [(0.0, 2, 0.0, False), (1.0, 1, 0.0, False)], 1: [(0.0, 2, 0.0, False)], 2: [(0.0, 2, 0.0, False), (1.0, 3, 0.0, True)], 3: [(0.0, 2, 0.0, False), (1.0, 9, 0.0, False)]}, 3: {0: [(0.0, 3, 0.0, False), (1.0, 2, 0.0, True)], 1: [(0.0, 3, 0.0, False)], 2: [(0.0, 3, 0.0, False), (1.0, 4, 0.0, True)], 3: [(0.0, 3, 0.0, False), (1.0, 10, 0.0, False)]}, 4: {0: [(0.0, 4, 0.0, False), (1.0, 3, 0.0, True)], 1: [(0.0, 4, 0.0, False)], 2: [(0.0, 4, 0.0, False), (1.0, 5, 0.0, False)], 3: [(0.0, 4, 0.0, False), (1.0, 11, 0.0, False)]}, 5: {0: [(0.0, 5, 0.0, False), (1.0, 4, 0.0, True)], 1: [(0.0, 5, 0.0, False)], 2: [(0.0, 5, 0.0, False), (1.0, 6, 0.0, False)], 3: [(0.0, 5, 0.0, False), (1.0, 12, 0.0, False)]}, 6: {0: [(0.0, 6, 0.0, False), (1.0, 5, 0.0, False)], 1: [(0.0, 6, 0.0, False)], 2: [(0.0, 6, 0.0, False)], 3: [(0.0, 6, 0.0, False), (1.0, 13, 0.0, False)]}, 7: {0: [(0.0, 7, 0.0, False)], 1: [(0.0, 7, 0.0, False), (1.0, 0, 0.0, False)], 2: [(0.0, 7, 0.0, False), (1.0, 8, 0.0, False)], 3: [(0.0, 7, 0.0, False), (1.0, 14, 0.0, False)]}, 8: {0: [(0.0, 8, 0.0, False), (1.0, 7, 0.0, False)], 1: [(0.0, 8, 0.0, False), (1.0, 1, 0.0, False)], 2: [(0.0, 8, 0.0, False), (1.0, 9, 0.0, False)], 3: [(0.0, 8, 0.0, False), (1.0, 15, 0.0, False)]}, 9: {0: [(0.0, 9, 0.0, False), (1.0, 8, 0.0, False)], 1: [(0.0, 9, 0.0, False), (1.0, 2, 0.0, True)], 2: [(0.0, 9, 0.0, False), (1.0, 10, 0.0, False)], 3: [(0.0, 9, 0.0, False), (1.0, 16, 0.0, False)]}, 10: {0: [(0.0, 10, 0.0, False), (1.0, 9, 0.0, False)], 1: [(0.0, 10, 0.0, False), (1.0, 3, 0.0, True)], 2: [(0.0, 10, 0.0, False), (1.0, 11, 0.0, False)], 3: [(0.0, 10, 0.0, False), (1.0, 17, 0.0, False)]}, 11: {0: [(0.0, 11, 0.0, False), (1.0, 10, 0.0, False)], 1: [(0.0, 11, 0.0, False), (1.0, 4, 0.0, True)], 2: [(0.0, 11, 0.0, False), (1.0, 12, 0.0, False)], 3: [(0.0, 11, 0.0, False), (1.0, 18, 0.0, False)]}, 12: {0: [(0.0, 12, 0.0, False), (1.0, 11, 0.0, False)], 1: [(0.0, 12, 0.0, False), (1.0, 5, 0.0, False)], 2: [(0.0, 12, 0.0, False), (1.0, 13, 0.0, False)], 3: [(0.0, 12, 0.0, False), (1.0, 19, 0.0, False)]}, 13: {0: [(0.0, 13, 0.0, False), (1.0, 12, 0.0, False)], 1: [(0.0, 13, 0.0, False), (1.0, 6, 0.0, False)], 2: [(0.0, 13, 0.0, False)], 3: [(0.0, 13, 0.0, False), (1.0, 20, 0.0, False)]}, 14: {0: [(0.0, 14, 0.0, False)], 1: [(0.0, 14, 0.0, False), (1.0, 7, 0.0, False)], 2: [(0.0, 14, 0.0, False), (1.0, 15, 0.0, False)], 3: [(0.0, 14, 0.0, False), (1.0, 21, 0.0, False)]}, 15: {0: [(0.0, 15, 0.0, False), (1.0, 14, 0.0, False)], 1: [(0.0, 15, 0.0, False), (1.0, 8, 0.0, False)], 2: [(0.0, 15, 0.0, False), (1.0, 16, 0.0, False)], 3: [(0.0, 15, 0.0, False), (1.0, 22, 0.0, False)]}, 16: {0: [(0.0, 16, 0.0, False), (1.0, 15, 0.0, False)], 1: [(0.0, 16, 0.0, False), (1.0, 9, 0.0, False)], 2: [(0.0, 16, 0.0, False), (1.0, 17, 0.0, False)], 3: [(0.0, 16, 0.0, False), (1.0, 23, 0.0, False)]}, 17: {0: [(0.0, 17, 0.0, False), (1.0, 16, 0.0, False)], 1: [(0.0, 17, 0.0, False), (1.0, 10, 0.0, False)], 2: [(0.0, 17, 0.0, False), (1.0, 18, 0.0, False)], 3: [(0.0, 17, 0.0, False), (1.0, 24, 0.0, False)]}, 18: {0: [(0.0, 18, 0.0, False), (1.0, 17, 0.0, False)], 1: [(0.0, 18, 0.0, False), (1.0, 11, 0.0, False)], 2: [(0.0, 18, 0.0, False), (1.0, 19, 0.0, False)], 3: [(0.0, 18, 0.0, False), (1.0, 25, 0.0, False)]}, 19: {0: [(0.0, 19, 0.0, False), (1.0, 18, 0.0, False)], 1: [(0.0, 19, 0.0, False), (1.0, 12, 0.0, False)], 2: [(0.0, 19, 0.0, False), (1.0, 20, 0.0, False)], 3: [(0.0, 19, 0.0, False), (1.0, 26, 0.0, False)]}, 20: {0: [(0.0, 20, 0.0, False), (1.0, 19, 0.0, False)], 1: [(0.0, 20, 0.0, False), (1.0, 13, 0.0, False)], 2: [(0.0, 20, 0.0, False)], 3: [(0.0, 20, 0.0, False), (1.0, 27, 0.0, False)]}, 21: {0: [(0.0, 21, 0.0, False)], 1: [(0.0, 21, 0.0, False), (1.0, 14, 0.0, False)], 2: [(0.0, 21, 0.0, False), (1.0, 22, 0.0, False)], 3: [(0.0, 21, 0.0, False), (1.0, 28, 0.0, False)]}, 22: {0: [(0.0, 22, 0.0, False), (1.0, 21, 0.0, False)], 1: [(0.0, 22, 0.0, False), (1.0, 15, 0.0, False)], 2: [(0.0, 22, 0.0, False), (1.0, 23, 0.0, False)], 3: [(0.0, 22, 0.0, False), (1.0, 29, 0.0, False)]}, 23: {0: [(0.0, 23, 0.0, False), (1.0, 22, 0.0, False)], 1: [(0.0, 23, 0.0, False), (1.0, 16, 0.0, False)], 2: [(0.0, 23, 0.0, False), (1.0, 24, 0.0, False)], 3: [(0.0, 23, 0.0, False), (1.0, 30, 0.0, False)]}, 24: {0: [(0.0, 24, 0.0, False), (1.0, 23, 0.0, False)], 1: [(0.0, 24, 0.0, False), (1.0, 17, 0.0, False)], 2: [(0.0, 24, 0.0, False), (1.0, 25, 0.0, False)], 3: [(0.0, 24, 0.0, False), (1.0, 31, 0.0, False)]}, 25: {0: [(0.0, 25, 0.0, False), (1.0, 24, 0.0, False)], 1: [(0.0, 25, 0.0, False), (1.0, 18, 0.0, False)], 2: [(0.0, 25, 0.0, False), (1.0, 26, 0.0, False)], 3: [(0.0, 25, 0.0, False), (1.0, 32, 0.0, False)]}, 26: {0: [(0.0, 26, 0.0, False), (1.0, 25, 0.0, False)], 1: [(0.0, 26, 0.0, False), (1.0, 19, 0.0, False)], 2: [(0.0, 26, 0.0, False), (1.0, 27, 0.0, False)], 3: [(0.0, 26, 0.0, False), (1.0, 33, 0.0, False)]}, 27: {0: [(0.0, 27, 0.0, False), (1.0, 26, 0.0, False)], 1: [(0.0, 27, 0.0, False), (1.0, 20, 0.0, False)], 2: [(0.0, 27, 0.0, False)], 3: [(0.0, 27, 0.0, False), (1.0, 34, 0.0, False)]}, 28: {0: [(0.0, 28, 0.0, False)], 1: [(0.0, 28, 0.0, False), (1.0, 21, 0.0, False)], 2: [(0.0, 28, 0.0, False), (1.0, 29, 0.0, False)], 3: [(0.0, 28, 0.0, False), (1.0, 35, 0.0, False)]}, 29: {0: [(0.0, 29, 0.0, False), (1.0, 28, 0.0, False)], 1: [(0.0, 29, 0.0, False), (1.0, 22, 0.0, False)], 2: [(0.0, 29, 0.0, False), (1.0, 30, 0.0, False)], 3: [(0.0, 29, 0.0, False), (1.0, 36, 0.0, False)]}, 30: {0: [(0.0, 30, 0.0, False), (1.0, 29, 0.0, False)], 1: [(0.0, 30, 0.0, False), (1.0, 23, 0.0, False)], 2: [(0.0, 30, 0.0, False), (1.0, 31, 0.0, False)], 3: [(0.0, 30, 0.0, False), (1.0, 37, 0.0, False)]}, 31: {0: [(0.0, 31, 0.0, False), (1.0, 30, 0.0, False)], 1: [(0.0, 31, 0.0, False), (1.0, 24, 0.0, False)], 2: [(0.0, 31, 0.0, False), (1.0, 32, 0.0, False)], 3: [(0.0, 31, 0.0, False), (1.0, 38, 0.0, False)]}, 32: {0: [(0.0, 32, 0.0, False), (1.0, 31, 0.0, False)], 1: [(0.0, 32, 0.0, False), (1.0, 25, 0.0, False)], 2: [(0.0, 32, 0.0, False), (1.0, 33, 0.0, False)], 3: [(0.0, 32, 0.0, False), (1.0, 39, 0.0, False)]}, 33: {0: [(0.0, 33, 0.0, False), (1.0, 32, 0.0, False)], 1: [(0.0, 33, 0.0, False), (1.0, 26, 0.0, False)], 2: [(0.0, 33, 0.0, False), (1.0, 34, 0.0, False)], 3: [(0.0, 33, 0.0, False), (1.0, 40, 0.0, False)]}, 34: {0: [(0.0, 34, 0.0, False), (1.0, 33, 0.0, False)], 1: [(0.0, 34, 0.0, False), (1.0, 27, 0.0, False)], 2: [(0.0, 34, 0.0, False)], 3: [(0.0, 34, 0.0, False), (1.0, 41, 0.0, False)]}, 35: {0: [(0.0, 35, 0.0, False)], 1: [(0.0, 35, 0.0, False), (1.0, 28, 0.0, False)], 2: [(0.0, 35, 0.0, False), (1.0, 36, 0.0, False)], 3: [(0.0, 35, 0.0, False)]}, 36: {0: [(0.0, 36, 0.0, False), (1.0, 35, 0.0, False)], 1: [(0.0, 36, 0.0, False), (1.0, 29, 0.0, False)], 2: [(0.0, 36, 0.0, False), (1.0, 37, 0.0, False)], 3: [(0.0, 36, 0.0, False)]}, 37: {0: [(0.0, 37, 0.0, False), (1.0, 36, 0.0, False)], 1: [(0.0, 37, 0.0, False), (1.0, 30, 0.0, False)], 2: [(0.0, 37, 0.0, False), (1.0, 38, 0.0, False)], 3: [(0.0, 37, 0.0, False)]}, 38: {0: [(0.0, 38, 0.0, False), (1.0, 37, 0.0, False)], 1: [(0.0, 38, 0.0, False), (1.0, 31, 0.0, False)], 2: [(0.0, 38, 0.0, False), (1.0, 39, 0.0, False)], 3: [(0.0, 38, 0.0, False)]}, 39: {0: [(0.0, 39, 0.0, False), (1.0, 38, 0.0, False)], 1: [(0.0, 39, 0.0, False), (1.0, 32, 0.0, False)], 2: [(0.0, 39, 0.0, False), (1.0, 40, 0.0, False)], 3: [(0.0, 39, 0.0, False)]}, 40: {0: [(0.0, 40, 0.0, False), (1.0, 39, 0.0, False)], 1: [(0.0, 40, 0.0, False), (1.0, 33, 0.0, False)], 2: [(0.0, 40, 0.0, False), (1.0, 41, 0.0, False)], 3: [(0.0, 40, 0.0, False)]}, 41: {0: [(0.0, 41, 0.0, False), (1.0, 40, 0.0, False)], 1: [(0.0, 41, 0.0, False), (1.0, 34, 0.0, False)], 2: [(0.0, 41, 0.0, False)], 3: [(0.0, 41, 0.0, False)]}}\n",
      "\n",
      "__\u001b[44m$\u001b[0m\u001b[44m$\u001b[0m\u001b[44m$\u001b[0m__\n",
      "___\u001b[47m#\u001b[0m___\n",
      "___\u001b[45m+\u001b[0m___\n",
      "_______\n",
      "\u001b[47m#\u001b[0m\u001b[47m#\u001b[0m_\u001b[47m#\u001b[0m\u001b[47m#\u001b[0m\u001b[47m#\u001b[0m\u001b[47m#\u001b[0m\n",
      "\u001b[41mo\u001b[0m_____\u001b[47m#\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def Qlearning(environment, num_episodes=100, alpha=0.3, gamma=0.9, epsilon=1.0, decay_epsilon=0.1, max_epsilon=1.0, min_epsilon=0.01):\r\n",
    "  \r\n",
    "  # initializing the Q-table\r\n",
    "  Q = np.zeros((environment.observation_space.n, environment.action_space.n))\r\n",
    "  \r\n",
    "  # additional lists to keep track of reward and epsilon values\r\n",
    "  rewards = []\r\n",
    "  epsilons = []\r\n",
    "\r\n",
    "  # episodes\r\n",
    "  for episode in range(num_episodes):\r\n",
    "      \r\n",
    "      # reset the environment to start a new episode\r\n",
    "      state = environment.reset()\r\n",
    "\r\n",
    "      # reward accumulated along episode\r\n",
    "      accumulated_reward = 0\r\n",
    "      \r\n",
    "      # steps within current episode\r\n",
    "      for step in range(100):\r\n",
    "          # epsilon-greedy action selection\r\n",
    "          # exploit with probability 1-epsilon\r\n",
    "          if np.random.uniform(0, 1) > epsilon:\r\n",
    "              action = np.argmax(Q[state,:])\r\n",
    "              \r\n",
    "          # explore with probability epsilon\r\n",
    "          else:\r\n",
    "              action = environment.action_space.sample()\r\n",
    "\r\n",
    "          # perform the action and observe the new state and corresponding reward\r\n",
    "          new_state, reward, done, info = environment.step(action)\r\n",
    "          \r\n",
    "\r\n",
    "          # update the Q-table\r\n",
    "          Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])\r\n",
    "          \r\n",
    "          # update the accumulated reward\r\n",
    "          accumulated_reward += reward\r\n",
    "\r\n",
    "          # update the current state\r\n",
    "          state = new_state\r\n",
    "\r\n",
    "          # end the episode when it is done\r\n",
    "          if done == True:\r\n",
    "              break\r\n",
    "      \r\n",
    "      # decay exploration rate to ensure that the agent exploits more as it becomes experienced\r\n",
    "      epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_epsilon*episode)\r\n",
    "      \r\n",
    "      # update the lists of rewards and epsilons\r\n",
    "      rewards.append(accumulated_reward)\r\n",
    "      epsilons.append(epsilon)\r\n",
    "\r\n",
    "  # render the environment\r\n",
    "  environment.render()\r\n",
    "    \r\n",
    "  # return the list of accumulated reward along episodes\r\n",
    "  return rewards"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "num_episodes=100\r\n",
    "alpha=0.3\r\n",
    "gamma=0.9\r\n",
    "epsilon=1.0\r\n",
    "decay_epsilon=0.1\r\n",
    "\r\n",
    "# run Q-learning\r\n",
    "rewards = Qlearning(t1_env, num_episodes, alpha, gamma, epsilon, decay_epsilon)\r\n",
    "\r\n",
    "# print results\r\n",
    "print (\"Average reward (all episodes): \" + str(sum(rewards)/num_episodes))\r\n",
    "print (\"Average reward (last 10 episodes): \" + str(sum(rewards[-10:])/10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (Left)\n",
      "__\u001b[44m$\u001b[0m\u001b[44m$\u001b[0m\u001b[44m$\u001b[0m__\n",
      "___\u001b[47m#\u001b[0m___\n",
      "___\u001b[45m+\u001b[0m___\n",
      "_______\n",
      "\u001b[47m#\u001b[0m\u001b[47m#\u001b[0m_\u001b[47m#\u001b[0m\u001b[47m#\u001b[0m\u001b[47m#\u001b[0m\u001b[47m#\u001b[0m\n",
      "\u001b[41mo\u001b[0m_____\u001b[47m#\u001b[0m\n",
      "Average reward (all episodes): 0.0\n",
      "Average reward (last 10 episodes): 0.0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('venv': virtualenv)"
  },
  "interpreter": {
   "hash": "4337733d7c4f8242ca10155413965bee5981b415db72db4cb080555abb5e32aa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}